{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the features matrix before dimensionality reduction: (100, 8865)\n",
      "\n",
      "Shape of the features matrix after dimensionality reduction: (100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load IMDB Movie Reviews Dataset\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Working with first 100 reviews\n",
    "first_100_reviewids = movie_reviews.fileids()[:100]\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in first_100_reviewids]\n",
    "\n",
    "# Transform raw data into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Apply TruncatedSVD for Dimensionality Reduction\n",
    "svd = TruncatedSVD(n_components=50)  \n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the features matrix before dimensionality reduction: (3, 10)\n",
      "\n",
      "Shape of the features matrix after dimensionality reduction: (3, 2)\n",
      "Reduced features matrix: \n",
      "[[ 0.81683228 -0.37215178]\n",
      " [ 0.87059126 -0.11198387]\n",
      " [ 0.455513    0.881375  ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The dog barked at the mail carrier\", \n",
    "             \"The mail carrier was afraid of the small dog\", \n",
    "             \"The small dog was not ferocious but it did like to bark\"]\n",
    "\n",
    "# Transform sentences into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# Apply TruncatedSVD on the TF-IDF matrix\n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the features matrix before dimensionality reduction: (3, 10)\n",
      "\n",
      "Shape of the features matrix after dimensionality reduction: (3, 2)\n",
      "Reduced features matrix: \n",
      "[[ 1.11616594  1.49849714]\n",
      " [ 1.94634624  0.37713888]\n",
      " [ 2.07711431 -1.15863355]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The cat meowed at the room window\", \n",
    "             \"The small kitten looked at the window and meowed\", \n",
    "             \"The ginger kitten didn't meow but it looked at the window\"]\n",
    "\n",
    "# Transform sentences into Bag-of-Words matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {bow_matrix.shape}\\n\")\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# Apply TruncatedSVD on the Bag-of-Words matrix\n",
    "features = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# Print shape after dimensionality reduction\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BoW shape: (3, 8)\n",
      "Shape after TruncatedSVD: (3, 2)\n",
      "Reduced features matrix: \n",
      "[[ 1.09544512e+00 -7.09511536e-18]\n",
      " [-6.70521600e-16  1.73205081e+00]\n",
      " [ 2.19089023e+00  7.63062702e-16]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "original_sentences = ['It is a lovely day, isn\\'t it?', \n",
    "                      'The sun is shining brightly!', \n",
    "                      'I love the taste of lemonade on a sunny day.']\n",
    "# Preprocess the sentences\n",
    "preprocessed_sentences = [clean_text(sentence) for sentence in original_sentences]\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit transform the preprocessed sentences\n",
    "X = vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "\n",
    "# Apply TruncatedSVD on the Bag-of-Words matrix\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "print(f\"Original BoW shape: {X.shape}\")\n",
    "print(f\"Shape after TruncatedSVD: {X_reduced.shape}\")\n",
    "print(f\"Reduced features matrix: \\n{X_reduced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the features matrix before dimensionality reduction: (150, 11138)\n",
      "\n",
      "Shape of the features matrix after dimensionality reduction: (150, 100)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# TODO: Load the IMDB Movie Reviews dataset\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "# TODO: Load the first 150 reviews from the dataset\n",
    "first_100_reviewids = movie_reviews.fileids()[:150]\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in first_100_reviewids]\n",
    "# TODO: Transform the movie reviews into a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)\n",
    "print(f\"Shape of the features matrix before dimensionality reduction: {tfidf_matrix.shape}\\n\")\n",
    "# TODO: Initialize and apply TruncatedSVD to the TF-IDF matrix\n",
    "svd = TruncatedSVD(n_components=100)  \n",
    "features = svd.fit_transform(tfidf_matrix)\n",
    "# TODO: Print the shape of the features matrix before and after applying TruncatedSVD\n",
    "print(f\"Shape of the features matrix after dimensionality reduction: {features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
