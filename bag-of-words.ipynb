{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['ball' 'cat' 'mat' 'near' 'on' 'played' 'sat' 'the' 'with']\n",
      "Bag of Words Representation:\n",
      "[[0 1 1 0 1 0 1 2 0]\n",
      " [0 1 1 1 0 0 1 2 0]\n",
      " [1 1 0 0 0 1 0 2 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Custom sentences\n",
    "sentences = ['The cat sat on the mat.',\n",
    "             'The cat sat near the mat.',\n",
    "             'The cat played with the ball.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['deep' 'fascinating' 'for' 'is' 'learning' 'machine' 'of' 'python'\n",
      " 'subset' 'use' 'we']\n",
      "Bag of Words Representation:\n",
      "[[0 1 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 1 2 1 1 0 1 0 0]\n",
      " [0 0 1 0 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple example sentences\n",
    "sentences = ['Machine learning is fascinating.',\n",
    "             'Deep learning is a subset of machine learning.',\n",
    "             'We use Python for machine learning.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ten feature names:  ['zingers' 'zipper' 'zombie' 'zombified' 'zone' 'zoologist' 'zoom' 'zwick'\n",
      " 'zwigoff' 'zzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# We need to download the dataset before we can use it\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "# Take only the first 100 reviews for simplicity\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()[:100]]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Print the last ten feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Last ten feature names: \", feature_names[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['at' 'concert' 'favorite' 'in' 'is' 'love' 'my' 'night' 'rain' 'sang'\n",
      " 'she' 'sing' 'singing' 'the' 'to' 'whole']\n",
      "Bag of Words Representation:\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 1 1 0 0 2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple sentences\n",
    "sentences = [\"I love to sing\", \n",
    "             \"Singing in the rain is my favorite\", \n",
    "             \"She sang the whole night at the concert\"]\n",
    "\n",
    "# TODO: Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# TODO: Fit transform the sentences\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['brightli' 'day' 'lemonad' 'love' 'shine' 'sun' 'sunni' 'tast']\n",
      "Bag of Words Representation:\n",
      "[[0 1 0 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0]\n",
      " [0 1 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load stop words from NLTK and initialize a stemmer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for text cleaning and stemming\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lower case\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\d', ' ', text)  # Remove digits\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "original_sentences = ['It is a lovely day, isn\\'t it?', \n",
    "                      'The sun is shining brightly!', \n",
    "                      'I love the taste of lemonade on a sunny day.']\n",
    "\n",
    "# Preprocess the sentences\n",
    "preprocessed_sentences = [clean_text(sentence) for sentence in original_sentences]\n",
    "\n",
    "# TODO: Initialize a CountVectorizer\n",
    "# TODO: Fit transform the preprocessed sentences\n",
    "# TODO: Print the feature names \n",
    "# TODO: Print the Bag of Words Representation\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# TODO: Fit transform the sentences\n",
    "X = vectorizer.fit_transform(preprocessed_sentences)\n",
    "print('Feature names:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Bag of Words Representation:')\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1, 10)\n",
      "Features: ['and' 'document' 'first' 'here' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Array: [[0.         0.61221452 0.         0.         0.30610726 0.\n",
      "  0.5865905  0.30610726 0.         0.30610726]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this document is the third one.',\n",
    "    'Is this the first document here?'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "vector = vectorizer.transform([sentences[1]])\n",
    "print('Shape:', vector.shape)\n",
    "print('Features:', vectorizer.get_feature_names_out())\n",
    "print('Array:', vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 8)\n",
      "Array: [[0.         0.52284231 0.         0.52284231 0.         0.\n",
      "  0.67325467 0.        ]\n",
      " [0.48359121 0.28561676 0.48359121 0.28561676 0.48359121 0.\n",
      "  0.36778358 0.        ]\n",
      " [0.         0.35959372 0.         0.35959372 0.         0.6088451\n",
      "  0.         0.6088451 ]]\n",
      "Data of Sparse Matrix  [0.52284231 0.67325467 0.52284231 0.48359121 0.48359121 0.48359121\n",
      " 0.28561676 0.36778358 0.28561676 0.6088451  0.6088451  0.35959372\n",
      " 0.35959372]\n",
      "Indices of non-zero elements of Sparse Matrix  [3 6 1 0 2 4 3 6 1 5 7 3 1]\n",
      "Pointer to start of each row in indices and data  [ 0  3  9 13]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = ['Expedition to Mars', 'NASA launched an expedition to Mars', 'Mars expedition was successful']\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "  \n",
    "# fit_transform to convert text to vector  \n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# print the shape and array\n",
    "print('Shape:', X.shape)\n",
    "print('Array:', X.toarray())\n",
    "\n",
    "# Print the data\n",
    "print(\"Data of Sparse Matrix \", X.data)\n",
    "\n",
    "# Get the indices of the non-zero elements \n",
    "print(\"Indices of non-zero elements of Sparse Matrix \", X.indices)\n",
    "\n",
    "# Get the array that points to where the start of each row is in the data and indices array\n",
    "print(\"Pointer to start of each row in indices and data \", X.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['zweibel' 'zwick' 'zwigoff' 'zycie' 'zzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download('movie_reviews', quiet=True)\n",
    "\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "\n",
    "# TODO: Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# TODO: Apply fit_transform on reviews\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "print('Feature Names:', vectorizer.get_feature_names_out()[-5:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
