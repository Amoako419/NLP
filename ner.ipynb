{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Microsoft', 'NNP'), ('Corporation', 'NNP'), ('aims', 'VBZ'), ('to', 'TO'), ('reach', 'VB'), ('95', 'CD'), ('global', 'JJ'), ('coverage', 'NN'), ('by', 'IN'), ('2025', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "\n",
    "# Given sentence\n",
    "sample_sentence = \"Microsoft Corporation aims to reach 95 global coverage by 2025\"\n",
    "\n",
    "# Get tokens\n",
    "tokens = word_tokenize(sample_sentence)\n",
    "\n",
    "# Get POS tags\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Perform NER\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "# Print the identified named entities\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Given sentence\n",
    "sample_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Get tokens\n",
    "tokens = word_tokenize(sample_sentence)\n",
    "\n",
    "# Get POS tags\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (ORGANIZATION CodeSignal/NNP)\n",
      "  is/VBZ\n",
      "  headquartered/VBN\n",
      "  in/IN\n",
      "  (GPE San/NNP Francisco/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "def apply_ner(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Apply POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Apply NER\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "    # Return the identified named entities\n",
    "    return named_entities\n",
    "\n",
    "# Apply the function to a specific sentence\n",
    "named_entities_in_sentence = apply_ner(\"CodeSignal is headquartered in San Francisco.\")\n",
    "print(named_entities_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The identified named entities : (S\n",
      "  I/PRP\n",
      "  was/VBD\n",
      "  wondering/VBG\n",
      "  if/IN\n",
      "  anyone/NN\n",
      "  out/IN\n",
      "  there/RB\n",
      "  could/MD\n",
      "  enlighten/VB\n",
      "  me/PRP\n",
      "  on/IN\n",
      "  this/DT\n",
      "  car/NN\n",
      "  I/PRP\n",
      "  saw/VBD\n",
      "  the/DT\n",
      "  other/JJ\n",
      "  day/NN\n",
      "  ./.\n",
      "  It/PRP\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  2-door/JJ\n",
      "  sports/NNS\n",
      "  car/NN\n",
      "  ,/,\n",
      "  looked/VBD\n",
      "  to/TO\n",
      "  be/VB\n",
      "  from/IN\n",
      "  the/DT\n",
      "  late/JJ\n",
      "  60s//CD\n",
      "  early/JJ\n",
      "  70s/CD\n",
      "  ./.\n",
      "  It/PRP\n",
      "  was/VBD\n",
      "  called/VBN\n",
      "  a/DT\n",
      "  (GPE Bricklin/NNP)\n",
      "  ./.\n",
      "  The/DT\n",
      "  doors/NNS\n",
      "  were/VBD\n",
      "  really/RB\n",
      "  small/JJ\n",
      "  ./.\n",
      "  In/IN\n",
      "  addition/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  front/NN\n",
      "  bumper/NN\n",
      "  was/VBD\n",
      "  separate/JJ\n",
      "  from/IN\n",
      "  the/DT\n",
      "  rest/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  body/NN\n",
      "  ./.\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  all/DT\n",
      "  I/PRP\n",
      "  know/VBP\n",
      "  ./.\n",
      "  If/IN\n",
      "  anyone/NN\n",
      "  can/MD\n",
      "  tellme/VB\n",
      "  a/DT\n",
      "  model/NN\n",
      "  name/NN\n",
      "  ,/,\n",
      "  engine/NN\n",
      "  specs/NN\n",
      "  ,/,\n",
      "  years/NNS\n",
      "  of/IN\n",
      "  production/NN\n",
      "  ,/,\n",
      "  where/WRB\n",
      "  this/DT\n",
      "  car/NN\n",
      "  is/VBZ\n",
      "  made/VBN\n",
      "  ,/,\n",
      "  history/NN\n",
      "  ,/,\n",
      "  or/CC\n",
      "  whatever/WDT\n",
      "  info/VBP\n",
      "  you/PRP\n",
      "  have/VBP\n",
      "  on/IN\n",
      "  this/DT\n",
      "  funky/NN\n",
      "  looking/VBG\n",
      "  car/NN\n",
      "  ,/,\n",
      "  please/VB\n",
      "  e-mail/JJ\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# TODO: Import the necessary NLTK modules for tokenization, POS tagging, and NER\n",
    "\n",
    "\n",
    "# Loading the data with metadata removed\n",
    "newsgroups_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# TODO: Extract the first document from the dataset\n",
    "sentence = newsgroups_data['data'][0]\n",
    "# TODO: Tokenize the text\n",
    "tokens = word_tokenize(sentence)\n",
    "# TODO: Apply POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "# TODO: Apply Named Entity Recognition\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "# TODO: Print the identified named entities\n",
    "print(f\"The identified named entities : {named_entities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
